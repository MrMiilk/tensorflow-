{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 供给数据\n",
    "- 在TensorFlow程序运行的每一步， 让Python代码来供给数据\n",
    "- 通过给run()或者eval()函数输入feed_dict参数， 可以启动运算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件读取\n",
    "- 在TensorFlow图的起始， 让一个输入管线从文件中读取数据\n",
    "- 文件读取的一般步骤\n",
    "    - 文件名列表\n",
    "    - 可配置的 文件名乱序(shuffling)\n",
    "    - 可配置的 最大训练迭代数(epoch limit)\n",
    "    - 文件名队列\n",
    "    - 针对输入文件格式的阅读器\n",
    "    - 纪录解析器\n",
    "    - 可配置的预处理器\n",
    "    - 样本队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 文件名列表\n",
    "- 使用python的列表\n",
    "```python\n",
    "['file1.csv', 'file2.csv', ]\n",
    "```\n",
    "- 使用tf.train.match_filenames_once\n",
    "![](imgs/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 文件名队列\n",
    "- 使用string_input_producer来生成一个先入先出的队列， 文件阅读器会需要它来读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 文件格式与对应的Reader, Decoder\n",
    "- CSV:TextLineReader, decode_csv\n",
    "- 二进制\n",
    "    - 从二进制文件中读取固定长度纪录， 可以使用tf.FixedLengthRecordReader的tf.decode_raw操作。decode_raw操作可以讲一个字符串转换为一个uint8的张量。\n",
    "\n",
    "    - 举例来说，the CIFAR-10 dataset的文件格式定义是：每条记录的长度都是固定的，一个字节的标签，后面是3072字节的图像数据。uint8的张量的标准操作就可以从中获取图像片并且根据需要进行重组。 例子代码可以在tensorflow/models/image/cifar10/cifar10_input.py找到，具体讲述可参见教程.\n",
    "- 标准TF格式(protocol buffer)\n",
    "    - 你可以写一段代码获取你的数据， 将数据填入到Example协议内存块(protocol buffer)，将协议内存块序列化为一个字符串， 并且通过tf.python_io.TFRecordWriter class写入到TFRecords文件，tensorflow/g3doc/how_tos/reading_data/convert_to_records.py就是这样的一个例子。\n",
    "    - 从TFRecords文件中读取数据， 可以使用tf.TFRecordReader的tf.parse_single_example解析器。这个parse_single_example操作可以将Example协议内存块(protocol buffer)解析为张量。 MNIST的例子就使用了convert_to_records 所构建的数据。 请参看tensorflow/g3doc/how_tos/reading_data/fully_connected_reader.py, 您也可以将这个例子跟fully_connected_feed的版本加以比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(['', ''])\n",
    "\n",
    "reader = tf.TextLinrReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [[1], [1], [1], [1], [1]]\n",
    "c1, c2, c3, c4, c5 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.concat(0, [c1, c2, c3])#维数\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Start populating the filename queue.协调器\n",
    "    coord = tf.train.Corrdinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)#这里还有默认的sess参数\n",
    "    \n",
    "    for i in range(1000):\n",
    "        example, label = sess.run([feature, c5])\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        每次read的执行都会从文件中读取一行内容， decode_csv 操作会解析这一行内容并将其转为张量列表。如果输入的参数有缺失，record_default参数可以根据张量的类型来设置默认值\n",
    "        在调用run或者eval去执行read之前， 你必须调用tf.train.start_queue_runners来将文件名填充到队列。否则read操作会被阻塞到文件名队列中有值为止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 预处理\n",
    "- 你可以对输入的样本进行任意的预处理， 这些预处理不依赖于训练参数， 你可以在tensorflow/models/image/cifar10/cifar10.py找到数据归一化， 提取随机数据片，增加噪声或失真等等预处理的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建\n",
    "- 在数据输入管线的末端， 我们需要有另一个队列来执行输入样本的训练，评价和推理。因此我们使用tf.train.shuffle_batch 函数来对队列中的样本进行乱序处理(参考文档的用法，以及P86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = some_decoder(record_string)\n",
    "    #数据处理\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #用于抽样的队列长度\n",
    "    #-- bigger means better shuffling but slower start up and more memory used.\n",
    "    # capacity 批数据队列容量\n",
    "    #   Recommendation(推荐):\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 1024\n",
    "    capcity = min_after_dequeue + 3*batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size, capcity, min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果你需要对不同文件中的样子有更强的乱序和并行处理，可以使用tf.train.shuffle_batch_join 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    #这里应该是由于是创建计算图，因而只是声明计算的线程\n",
    "    example_list = [read_my_file_fromat(filename_queue)\n",
    "                   for _ in range(read_threads)]\n",
    "    min_after_dequeue = 1024\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batche = tf.train.shuffle_batch_join(\n",
    "        example_list, batch_size, capacity, min_after_dequeue)\n",
    "    \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另一种替代方案是： 使用tf.train.shuffle_batch 函数,设置num_threads的值大于1。 这种方案可以保证同一时刻只在一个文件中进行读取操作(但是读取速度依然优于单线程)，而不是之前的同时读取多个文件。这种方案的优点是：\n",
    "\n",
    "    - 避免了两个不同的线程从同一个文件中读取同一个样本。\n",
    "    - 避免了过多的磁盘搜索操作。\n",
    "- 你一共需要多少个读取线程呢？ 函数tf.train.shuffle_batch*为TensorFlow图提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考[TensorBoard:可视化学习](http://www.tensorfly.cn/tfdoc/how_tos/summaries_and_tensorboard.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 启动，运用\n",
    "- 简单来说：使用上面列出的许多tf.train函数添加QueueRunner到你的数据流图中。在你运行任何训练步骤之前，需要调用tf.train.start_queue_runners函数，否则数据流图将一直挂起。tf.train.start_queue_runners 这个函数将会启动输入管道的线程，填充样本到队列中，以便出队操作可以从队列中拿到样本。这种情况下最好配合使用一个tf.train.Coordinator，这样可以在发生错误的情况下正确地关闭这些线程。如果你对训练迭代数做了限制，那么需要使用一个训练迭代数计数器，并且需要被初始化\n",
    "- 以下为文档推荐代码(P85也有)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph, etc.\n",
    "init_op = tf.initialize_all_variables()\n",
    "'''或者聚合初始化(P85)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                    tf.local_variables_initializer())#local的是使用协调器要求的\n",
    "'''\n",
    "# Create a session for running operations in the Graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize the variables (like the epoch counter).\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# Start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "try:\n",
    "    while not coord.should_stop:\n",
    "        #run training steps or whatever\n",
    "        sess.run(train_op)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "    \n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：（参见官方文档）\n",
    "-  在达到最大训练迭代数的时候如何清理关闭线程?\n",
    "- 筛选记录或产生每个记录的多个样本\n",
    "- 稀疏输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预加载数据\n",
    "- 这仅用于可以完全加载到存储器中的小的数据集。有两种方法：\n",
    "\n",
    "    - 存储在常数中。\n",
    "    - 存储在变量中，初始化后，永远不要改变它的值。\n",
    "- 使用常数更简单一些，但是会使用更多的内存（因为常数会内联的存储在数据流图数据结构中，这个结构体可能会被复制几次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#常量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session():\n",
    "    input_data = tf.constant(training_data)\n",
    "    input_labels = tf.constant(training_labels)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session() as sess:\n",
    "    data_initializer = tf.placeholder(dtype=training_data.dtype,\n",
    "                                    shape=training_data.shape)\n",
    "    label_initializer = tf.placeholder(dtype=training_labels.dtype,\n",
    "                                     shape=training_labels.shape)\n",
    "    input_data = tf.Variable(data_initalizer, trainable=False, collections=[])\n",
    "    input_labels = tf.Variable(label_initalizer, trainable=False, collections=[])\n",
    "    ...\n",
    "    sess.run(input_data.initializer,\n",
    "           feed_dict={data_initializer: training_data})\n",
    "    sess.run(input_labels.initializer,\n",
    "           feed_dict={label_initializer: training_lables})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设定trainable=False 可以防止该变量被数据流图的 GraphKeys.TRAINABLE_VARIABLES 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 collections=[] 可以防止GraphKeys.VARIABLES 收集后做为保存和恢复的中断点。\n",
    "\n",
    "- 无论哪种方式，[tf.train.slice_input_producer function](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#slice_input_producer)函数可以被用来每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用shuffle_batch函数，取而代之的是纯[tf.train.batch](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#batch) 函数。 如果要使用多个线程进行预处理，需要将num_threads参数设置为大于1的数字。\n",
    "\n",
    "- 在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded.py 中可以找到一个MNIST例子，使用常数来预加载。 另外使用变量来预加载的例\n",
    "子在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded_var.py，你可以用上面 fully_connected_feed 和 fully_connected_reader 的描述来进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
