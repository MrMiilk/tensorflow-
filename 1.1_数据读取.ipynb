{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 供给数据\n",
    "- 对于小型数据，直接读入内存，而大型数据通过‘输入流水线’的方式读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入流水线\n",
    "- 创建文件名列表\n",
    "- 创建文件名队列\n",
    "- 创建Reader和Decoder\n",
    "- 创建样例队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 文件名列表\n",
    "- 使用python的列表\n",
    "```python\n",
    "['file1.csv', 'file2.csv', ]\n",
    "```\n",
    "- 使用tf.train.match_filenames_once\n",
    "    - 会在流图中创建一个获取文件名列表的操作(operation)，文件名了列表会在全局初始化时一起被初始化\n",
    "![](imgs/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 文件名队列\n",
    "- 使用string_input_producer来生成一个先入先出的队列(FIFO)， 文件阅读器会需要它来读取数据\n",
    "    - **通过设置num_epochs(最大训练周期),shuffle,等进行调节**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 文件格式与对应的Reader, Decoder\n",
    "- Reader的read方法进行相应的读取，然后Decoder将其转化为张量形式\n",
    "- CSV:TextLineReader, decode_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ReaderReadUpToV2_5:1\", shape=(?,), dtype=string)\n",
      "Tensor(\"stack_15:0\", shape=(3, ?), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(['datasets/dat99.csv', ])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read_up_to(filename_queue, 5)\n",
    "print(value)\n",
    "record_defaults = [['B'], ['F'], [1], [1], [1], ['BJ'], [1.], [1], [1.]]\n",
    "c1, c2, c3, c4, c5, c6, c7, c8, c9 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([c1, c2, c6])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用read_up_to()返回列表 (keys, values) ,通过tf.stack()组合需要的数据\n",
    "    - 如果是reader：每次read的执行都会从文件中读取一行内容\n",
    "- decode_csv 操作会解析内容并将其转为张量列表。如果输入的参数有缺失，record_default参数可以根据张量的类型来设置默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'TJ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'TJ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'TJ'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'CQ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SZ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(['datasets/dat99.csv'])\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "\n",
    "record_defaults = [['B'], ['F'], [1.], [1.], [1.], ['BJ'], [1.], [1.], [1.]]\n",
    "c1, c2, c3, c4, c5, c6, c7, c8, c9 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([c1, c2, c6])#维数\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 协调器\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        example, label = sess.run([features, c5])\n",
    "        print(example, label)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在调用run或者eval去执行read之前， 你必须调用tf.train.start_queue_runners来将文件名填充到队列。否则read操作会被阻塞到文件名队列中有值为止\n",
    "- 注意一些csv文件有标题，需要去掉，可以参考文档的Reader类\n",
    "- 速度很快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 标准TF格式(protocol buffer)，TFRecord\n",
    "    - TFRecoder数据类型是通过tf.train.Example 格式存储的，这种格式包含了从属性名称到取值的字典，其中名称为字符串类型，取值可以是ByteList,Int64List,FloatList，结构声明在P86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二进制\n",
    "    - 从二进制文件中读取固定长度纪录， 可以使用tf.FixedLengthRecordReader的tf.decode_raw操作。decode_raw操作可以讲一个字符串转换为一个uint8的张量。\n",
    "\n",
    "    - 举例来说，the CIFAR-10 dataset的文件格式定义是：每条记录的长度都是固定的，一个字节的标签，后面是3072字节的图像数据。uint8的张量的标准操作就可以从中获取图像片并且根据需要进行重组。 例子代码可以在tensorflow/models/image/cifar10/cifar10_input.py找到，具体讲述可参见教程."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 创建样例队列\n",
    "- 经过上面的步骤得到的是张量\n",
    "- 通过调用tf.train.start_queue_runners()启动线程进行读取，这里还需要一个协调器，不然会出现线程不协调导致的越界之类的错误，同时，**启动协调器必须执行tf.local_variables_initializer()对其进行初始化.参考一下上面的程序，没有进行初始化，但是并没有出错，不过书中的程序不同，在具体使用时再参考**，书上的使用如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph, etc.\n",
    "init_op = tf.initialize_all_variables()\n",
    "'''或者聚合初始化(P85)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                    tf.local_variables_initializer())#local的是使用协调器要求的\n",
    "'''\n",
    "# Create a session for running operations in the Graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize the variables (like the epoch counter).\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# Start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#相比直接使用，加入出错判断，这种应该会更加适合实际使用\n",
    "try:\n",
    "    while not coord.should_stop:\n",
    "        #run training steps or whatever\n",
    "        sess.run(train_op)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "    \n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下给出两个来自别的书上TFRecord数据类型的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist_data\\train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#mnist转化为TFRecord\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "mnist = input_data.read_data_sets(\"datasets/mnist_data\", dtype=tf.uint8, one_hot=True)\n",
    "images = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "pixels = images.shape[1]\n",
    "num_examples = mnist.train.num_examples\n",
    "\n",
    "filename = \"datasets/mnist.tfrecords\"\n",
    "#创建writer用于写入\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for index in range(num_examples):\n",
    "    image_row = images[index].tostring()#这里将image编码成byte格式\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'pixels': _int64_feature(pixels),\n",
    "        'label': _int64_feature(np.argmax(labels[index])),\n",
    "        'image': _bytes_feature(image_row)\n",
    "    }))\n",
    "    #写入文件\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#相应的读取\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['datasets/mnist.tfrecords', ])\n",
    "#这里和之前csv读取的例子一样，也可以使用read_up_to()\n",
    "_, value = reader.read(filename_queue)\n",
    "#如果是多个样例，这里应该使用parse_example()\n",
    "features = tf.parse_single_example(\n",
    "    value,\n",
    "    features={\n",
    "        #这里解析方法还有tf.VarLenFeature(),结果是稀疏张量\n",
    "        'image': tf.FixedLenFeature([], tf.string),#参数是shape，dtype，default_value\n",
    "        'pixels': tf.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "#tf.decode_raw()可以将字符串解析成图像\n",
    "image_ = tf.decode_raw(features['image'], tf.uint8)\n",
    "label_ = tf.cast(features['label'], tf.int32)\n",
    "pixels_ = tf.cast(features['pixels'], tf.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for i in range(10):\n",
    "        image, label, pixels = sess.run([image_, label_, pixels_])\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 另一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据\n",
    "import tensorflow as tf\n",
    "\n",
    "#TFRecoder文件的帮助函数\n",
    "def _int64_feature(values):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "#将数据写入不同文件\n",
    "num_shareds = 2 #文件数\n",
    "instance_per_shard = 2 #文件有多少数据\n",
    "\n",
    "for i in range(num_shareds):\n",
    "    # 按0000n-of-0000m的后缀区分文件。n代表当前文件编号，m代表文件总数\n",
    "    filename = ('data/data.tfrecords-%.5d-of-%.5d' % (i, num_shareds))\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    \n",
    "    #数据封装成Example结构，写入TFRecoder文件\n",
    "    for j in range(instances_per_shard):\n",
    "        example = tf.train.Example(\n",
    "            features = tf.train.Features(feature={\n",
    "                'i':_int64_feature(i),\n",
    "                'j':_int64_feature(j)\n",
    "            }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 获取文件列表\n",
    "files = tf.train.match_filenames_once('data/data.tfrecords-*')\n",
    "\n",
    "# 创建文件输入队列\n",
    "filename_queue = tf.train.string_input_producer(files, shuffle=False)#再次注意，这里可以设置训练周期和shuffle\n",
    "\n",
    "# 读取并解析Example\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 使用match_filenames_once需要用local_variables_initializer初始化其中的一些变量，不然会出错\n",
    "    sess.run(\n",
    "        [tf.global_variables_initializer(),\n",
    "         tf.local_variables_initializer()])\n",
    "    '''或者聚合初始化(P85)\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                        tf.local_variables_initializer())#local的是使用协调器要求的\n",
    "    '''\n",
    "    # 打印文件名\n",
    "    print(sess.run(files))\n",
    "\n",
    "    # 用Coordinator协同线程，并启动线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # 获取数据\n",
    "    for i in range(6):\n",
    "        print(sess.run([features['i'], features['j']]))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 预处理\n",
    "- 对输入的样本进行任意的预处理， 这些预处理不依赖于训练参数， 在tensorflow/models/image/cifar10/cifar10.py可以找到数据归一化， 提取随机数据片，增加噪声或失真等等预处理的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 至此，已经实现上面动图中的example Queue，在P87，作者提出实际应用时还需要一个batch Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 批处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建\n",
    "- 在数据输入管线的末端， 我们需要有另一个队列来执行输入样本的训练，评价和推理。因此我们使用tf.train.shuffle_batch 函数来对队列中的样本进行乱序处理(参考文档的用法，以及P86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    \"\"\"这部分就是前面解决的问题\"\"\"\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = some_decoder(record_string)\n",
    "    #数据预处理\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True\n",
    "    )\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    #以下为创建batch Queue，注意这里batch Queue类似插入在喂食到网络前的预处理层\n",
    "    min_after_dequeue = 1024\n",
    "    capcity = min_after_dequeue + 3*batch_size\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample(用于抽样的队列长度)\n",
    "    #        -- bigger means better shuffling but slower start up and more memory used.\n",
    "    # capacity 批数据队列容量\n",
    "    #        Recommendation(推荐):\n",
    "    #          min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size, capcity, min_after_dequeue\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这里还有一个类似的tf.train.shuffle_batch_join 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True\n",
    "    )\n",
    "    #使用多线程处理\n",
    "    example_list = [read_my_file_fromat(filename_queue)\n",
    "                   for _ in range(read_threads)]\n",
    "    min_after_dequeue = 1024\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batche = tf.train.shuffle_batch_join(\n",
    "        example_list, batch_size, capacity, min_after_dequeue\n",
    "    )\n",
    "    \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另一种替代方案是： 使用tf.train.shuffle_batch 函数,设置num_threads的值大于1。 这种方案可以保证同一时刻只在一个文件中进行读取操作(但是读取速度依然优于单线程)，而不是之前的同时读取多个文件。这种方案的优点是：\n",
    "\n",
    "    - 避免了两个不同的线程从同一个文件中读取同一个样本。\n",
    "    - 避免了过多的磁盘搜索操作。\n",
    "- 你一共需要多少个读取线程呢？ 函数tf.train.shuffle_batch*为TensorFlow图提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考[TensorBoard:可视化学习](http://www.tensorfly.cn/tfdoc/how_tos/summaries_and_tensorboard.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型中的具体调用模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = input_pipline()\n",
    "with tf.Session as sess:\n",
    "    init_op = tf.group([\n",
    "        tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer()\n",
    "    ])\n",
    "    sess.run(init_op)\n",
    "    #协调器。现在代码有较好的封装\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    try:\n",
    "        for _ in range(1000):\n",
    "            if not coord.should_stop():\n",
    "                sess.run(train_op)\n",
    "                print('xx')\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('OutOfRangeError')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        print('Finish reading')\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：（参见官方文档）\n",
    "-  在达到最大训练迭代数的时候如何清理关闭线程?\n",
    "- 筛选记录或产生每个记录的多个样本\n",
    "- 稀疏输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预加载数据\n",
    "- 这仅用于可以完全加载到存储器中的小的数据集。有两种方法：\n",
    "\n",
    "    - 存储在常数中。\n",
    "    - 存储在变量中，初始化后，永远不要改变它的值。\n",
    "- 使用常数更简单一些，但是会使用更多的内存（因为常数会内联的存储在数据流图数据结构中，这个结构体可能会被复制几次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#常量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session():\n",
    "    input_data = tf.constant(training_data)\n",
    "    input_labels = tf.constant(training_labels)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session() as sess:\n",
    "    data_initializer = tf.placeholder(dtype=training_data.dtype,\n",
    "                                    shape=training_data.shape)\n",
    "    label_initializer = tf.placeholder(dtype=training_labels.dtype,\n",
    "                                     shape=training_labels.shape)\n",
    "    input_data = tf.Variable(data_initalizer, trainable=False, collections=[])\n",
    "    input_labels = tf.Variable(label_initalizer, trainable=False, collections=[])\n",
    "    ...\n",
    "    sess.run(input_data.initializer,\n",
    "           feed_dict={data_initializer: training_data})\n",
    "    sess.run(input_labels.initializer,\n",
    "           feed_dict={label_initializer: training_lables})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设定trainable=False 可以防止该变量被数据流图的 GraphKeys.TRAINABLE_VARIABLES 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 collections=[] 可以防止GraphKeys.VARIABLES 收集后做为保存和恢复的中断点。\n",
    "\n",
    "- 无论哪种方式，[tf.train.slice_input_producer function](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#slice_input_producer)函数可以被用来每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用shuffle_batch函数，取而代之的是纯[tf.train.batch](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#batch) 函数。 如果要使用多个线程进行预处理，需要将num_threads参数设置为大于1的数字。\n",
    "#### 例子\n",
    "- 在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded.py 中可以找到一个MNIST例子，使用常数来预加载。 另外使用变量来预加载的例子\n",
    "- 在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded_var.py，你可以用上面 fully_connected_feed 和 fully_connected_reader 的描述来进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 多输入通道\n",
    "- 通常你会在一个数据集上面训练，然后在另外一个数据集上做评估计算(或称为 \"eval\")。 这样做的一种方法是，实际上包含两个独立的进程：\n",
    "\n",
    "    - 训练过程中读取输入数据，并定期将所有的训练的变量写入还原点文件）。\n",
    "    - 在计算过程中恢复还原点文件到一个推理模型中，读取有效的输入数据。\n",
    "- 这两个进程在下面的例子中已经完成了：the example CIFAR-10 model，有以下几个好处：\n",
    "\n",
    "    - eval被当做训练后变量的一个简单映射。\n",
    "    - 你甚至可以在训练完成和退出后执行eval。\n",
    "- 您可以在同一个进程的相同的数据流图中有训练和eval，并分享他们的训练后的变量。参考the shared variables tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
