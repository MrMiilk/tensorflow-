{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 供给数据\n",
    "- 对于小型数据，直接读入内存，而大型数据通过‘输入流水线’的方式读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入流水线\n",
    "- 创建文件名列表\n",
    "- 创建文件名队列\n",
    "- 创建Reader和Decoder\n",
    "- 创建样例队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 文件名列表\n",
    "- 使用python的列表\n",
    "```python\n",
    "['file1.csv', 'file2.csv', ]\n",
    "```\n",
    "- 使用tf.train.match_filenames_once\n",
    "    - 会在流图中创建一个获取文件名列表的操作(operation)，文件名了列表会在全局初始化时一起被初始化\n",
    "![](imgs/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 文件名队列\n",
    "- 使用string_input_producer来生成一个先入先出的队列(FIFO)， 文件阅读器会需要它来读取数据\n",
    "    - **通过设置num_epochs(最大训练周期),shuffle,等进行调节**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 文件格式与对应的Reader, Decoder\n",
    "- Reader的read方法进行相应的读取，然后Decoder将其转化为张量形式\n",
    "- CSV:TextLineReader, decode_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ReaderReadUpToV2_5:1\", shape=(?,), dtype=string)\n",
      "Tensor(\"stack_15:0\", shape=(3, ?), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(['datasets/dat99.csv', ])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read_up_to(filename_queue, 5)\n",
    "print(value)\n",
    "record_defaults = [['B'], ['F'], [1], [1], [1], ['BJ'], [1.], [1], [1.]]\n",
    "c1, c2, c3, c4, c5, c6, c7, c8, c9 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([c1, c2, c6])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用read_up_to()返回列表 (keys, values) ,通过tf.stack()组合需要的数据\n",
    "    - 如果是reader：每次read的执行都会从文件中读取一行内容\n",
    "- decode_csv 操作会解析内容并将其转为张量列表。如果输入的参数有缺失，record_default参数可以根据张量的类型来设置默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'TJ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 1.0\n",
      "[b'B' b'F' b'SH'] 1.0\n",
      "[b'B' b'F' b'SZ'] 1.0\n",
      "[b'B' b'F' b'TJ'] 1.0\n",
      "[b'B' b'F' b'CQ'] 1.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 2.0\n",
      "[b'B' b'F' b'SH'] 2.0\n",
      "[b'B' b'F' b'SZ'] 2.0\n",
      "[b'B' b'F' b'TJ'] 2.0\n",
      "[b'B' b'F' b'CQ'] 2.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 3.0\n",
      "[b'B' b'F' b'SH'] 3.0\n",
      "[b'B' b'F' b'SZ'] 3.0\n",
      "[b'B' b'F' b'TJ'] 3.0\n",
      "[b'B' b'F' b'CQ'] 3.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 4.0\n",
      "[b'B' b'F' b'SH'] 4.0\n",
      "[b'B' b'F' b'SZ'] 4.0\n",
      "[b'B' b'F' b'TJ'] 4.0\n",
      "[b'B' b'F' b'CQ'] 4.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 5.0\n",
      "[b'B' b'F' b'SH'] 5.0\n",
      "[b'B' b'F' b'SZ'] 5.0\n",
      "[b'B' b'F' b'TJ'] 5.0\n",
      "[b'B' b'F' b'CQ'] 5.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'SZ'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'TJ'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 6.0\n",
      "[b'B' b'F' b'SH'] 6.0\n",
      "[b'B' b'F' b'CQ'] 6.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'TJ'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'BJ'] 7.0\n",
      "[b'B' b'F' b'SH'] 7.0\n",
      "[b'B' b'F' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 1.0\n",
      "[b'B' b'M' b'SH'] 1.0\n",
      "[b'B' b'M' b'SZ'] 1.0\n",
      "[b'B' b'M' b'TJ'] 1.0\n",
      "[b'B' b'M' b'CQ'] 1.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 2.0\n",
      "[b'B' b'M' b'SH'] 2.0\n",
      "[b'B' b'M' b'SZ'] 2.0\n",
      "[b'B' b'M' b'TJ'] 2.0\n",
      "[b'B' b'M' b'CQ'] 2.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 3.0\n",
      "[b'B' b'M' b'SH'] 3.0\n",
      "[b'B' b'M' b'SZ'] 3.0\n",
      "[b'B' b'M' b'TJ'] 3.0\n",
      "[b'B' b'M' b'CQ'] 3.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 4.0\n",
      "[b'B' b'M' b'SH'] 4.0\n",
      "[b'B' b'M' b'SZ'] 4.0\n",
      "[b'B' b'M' b'TJ'] 4.0\n",
      "[b'B' b'M' b'CQ'] 4.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 5.0\n",
      "[b'B' b'M' b'SH'] 5.0\n",
      "[b'B' b'M' b'SZ'] 5.0\n",
      "[b'B' b'M' b'TJ'] 5.0\n",
      "[b'B' b'M' b'CQ'] 5.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 6.0\n",
      "[b'B' b'M' b'SH'] 6.0\n",
      "[b'B' b'M' b'SZ'] 6.0\n",
      "[b'B' b'M' b'TJ'] 6.0\n",
      "[b'B' b'M' b'CQ'] 6.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'B' b'M' b'BJ'] 7.0\n",
      "[b'B' b'M' b'SH'] 7.0\n",
      "[b'B' b'M' b'SZ'] 7.0\n",
      "[b'B' b'M' b'TJ'] 7.0\n",
      "[b'B' b'M' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'CQ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SZ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'SH'] 1.0\n",
      "[b'C' b'F' b'SZ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'TJ'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'TJ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'CQ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'BJ'] 6.0\n",
      "[b'C' b'F' b'SH'] 6.0\n",
      "[b'C' b'F' b'SZ'] 6.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'TJ'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'SH'] 7.0\n",
      "[b'C' b'F' b'CQ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 7.0\n",
      "[b'C' b'F' b'BJ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'TJ'] 1.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'CQ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 2.0\n",
      "[b'C' b'F' b'SH'] 2.0\n",
      "[b'C' b'F' b'SZ'] 2.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 3.0\n",
      "[b'C' b'F' b'SH'] 3.0\n",
      "[b'C' b'F' b'SZ'] 3.0\n",
      "[b'C' b'F' b'TJ'] 3.0\n",
      "[b'C' b'F' b'CQ'] 3.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 4.0\n",
      "[b'C' b'F' b'SH'] 4.0\n",
      "[b'C' b'F' b'SZ'] 4.0\n",
      "[b'C' b'F' b'TJ'] 4.0\n",
      "[b'C' b'F' b'CQ'] 4.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n",
      "[b'C' b'F' b'CQ'] 5.0\n",
      "[b'C' b'F' b'BJ'] 5.0\n",
      "[b'C' b'F' b'SH'] 5.0\n",
      "[b'C' b'F' b'SZ'] 5.0\n",
      "[b'C' b'F' b'TJ'] 5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(['datasets/dat99.csv'])\n",
    "\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "\n",
    "record_defaults = [['B'], ['F'], [1.], [1.], [1.], ['BJ'], [1.], [1.], [1.]]\n",
    "c1, c2, c3, c4, c5, c6, c7, c8, c9 = tf.decode_csv(\n",
    "    value, record_defaults=record_defaults)\n",
    "features = tf.stack([c1, c2, c6])#维数\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 协调器\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        example, label = sess.run([features, c5])\n",
    "        print(example, label)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在调用run或者eval去执行read之前， 你必须调用tf.train.start_queue_runners来将文件名填充到队列。否则read操作会被阻塞到文件名队列中有值为止\n",
    "- 注意一些csv文件有标题，需要去掉，可以参考文档的Reader类\n",
    "- 速度很快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 标准TF格式(protocol buffer)，TFRecord\n",
    "    - TFRecoder数据类型是通过tf.train.Example 格式存储的，这种格式包含了从属性名称到取值的字典，其中名称为字符串类型，取值可以是ByteList,Int64List,FloatList，结构声明在P86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二进制\n",
    "    - 从二进制文件中读取固定长度纪录， 可以使用tf.FixedLengthRecordReader的tf.decode_raw操作。decode_raw操作可以讲一个字符串转换为一个uint8的张量。\n",
    "\n",
    "    - 举例来说，the CIFAR-10 dataset的文件格式定义是：每条记录的长度都是固定的，一个字节的标签，后面是3072字节的图像数据。uint8的张量的标准操作就可以从中获取图像片并且根据需要进行重组。 例子代码可以在tensorflow/models/image/cifar10/cifar10_input.py找到，具体讲述可参见教程."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 创建样例队列\n",
    "- 经过上面的步骤得到的是张量\n",
    "- 通过调用tf.train.start_queue_runners()启动线程进行读取，这里还需要一个协调器，不然会出现线程不协调导致的越界之类的错误，同时，**启动协调器必须执行tf.local_variables_initializer()对其进行初始化.参考一下上面的程序，没有进行初始化，但是并没有出错，不过书中的程序不同，在具体使用时再参考**，书上的使用如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the graph, etc.\n",
    "init_op = tf.initialize_all_variables()\n",
    "'''或者聚合初始化(P85)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                    tf.local_variables_initializer())#local的是使用协调器要求的\n",
    "'''\n",
    "# Create a session for running operations in the Graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize the variables (like the epoch counter).\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# Start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#相比直接使用，加入出错判断，这种应该会更加适合实际使用\n",
    "try:\n",
    "    while not coord.should_stop:\n",
    "        #run training steps or whatever\n",
    "        sess.run(train_op)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "    \n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下给出两个来自别的书上TFRecord数据类型的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1318\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[1;32m-> 1400\u001b[1;33m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[0;32m   1401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    406\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                          _context=self, _session=session)\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\u001b[0m\n\u001b[0;32m    813\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1067\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-588368a02abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/mnist_data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[1;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[1;32m--> 260\u001b[1;33m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[0;32m    261\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[0;32m    217\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m               instructions)\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mdelay\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdelays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_retriable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\u001b[0m in \u001b[0;36murlretrieve_with_retry\u001b[1;34m(url, filename)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_retriable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_is_retriable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0murlretrieve_with_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1361\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sx352\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>"
     ]
    }
   ],
   "source": [
    "#mnist转化为TFRecord\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "mnist = input_data.read_data_sets(\"datasets/mnist_data\", dtype=tf.uint8, one_hot=True)\n",
    "images = mnist.train.images\n",
    "labels = mnist.train.labels\n",
    "pixels = images.shape[1]\n",
    "num_examples = mnist.train.num_examples\n",
    "\n",
    "filename = \"datasets/mnist.tfrecords\"\n",
    "#创建writer用于写入\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for index in range(num_examples):\n",
    "    image_row = images[index].tostring()#这里将image编码成byte格式\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'pixels': _int64_feature(pixels),\n",
    "        'label': _int64_feature(np.argmax(labels[index])),\n",
    "        'image': _bytes_feature(image_row)\n",
    "    }))\n",
    "    #写入文件\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b1e81ea52928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#相应的读取\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFRecordReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfilename_queue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_input_producer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datasets/mnist.tfrecords'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#这里和之前csv读取的例子一样，也可以使用read_up_to()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#相应的读取\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['datasets/mnist.tfrecords', ])\n",
    "#这里和之前csv读取的例子一样，也可以使用read_up_to()\n",
    "_, value = reader.read(filename_queue)\n",
    "#如果是多个样例，这里应该使用parse_example()\n",
    "features = tf.parse_single_example(\n",
    "    value,\n",
    "    features={\n",
    "        #这里解析方法还有tf.VarLenFeature(),结果是稀疏张量\n",
    "        'image': tf.FixedLenFeature([], tf.string),#参数是shape，dtype，default_value\n",
    "        'pixels': tf.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "#tf.decode_raw()可以将字符串解析成图像\n",
    "image_ = tf.decode_raw(features['image'], tf.uint8)##一定要注意这里的数据类型\n",
    "label_ = tf.cast(features['label'], tf.int32)\n",
    "pixels_ = tf.cast(features['pixels'], tf.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for i in range(10):\n",
    "        image, label, pixels = sess.run([image_, label_, pixels_])\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 另一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成数据\n",
    "import tensorflow as tf\n",
    "\n",
    "#TFRecoder文件的帮助函数\n",
    "def _int64_feature(values):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "#将数据写入不同文件\n",
    "num_shareds = 2 #文件数\n",
    "instance_per_shard = 2 #文件有多少数据\n",
    "\n",
    "for i in range(num_shareds):\n",
    "    # 按0000n-of-0000m的后缀区分文件。n代表当前文件编号，m代表文件总数\n",
    "    filename = ('data/data.tfrecords-%.5d-of-%.5d' % (i, num_shareds))\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    \n",
    "    #数据封装成Example结构，写入TFRecoder文件\n",
    "    for j in range(instances_per_shard):\n",
    "        example = tf.train.Example(\n",
    "            features = tf.train.Features(feature={\n",
    "                'i':_int64_feature(i),\n",
    "                'j':_int64_feature(j)\n",
    "            }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 获取文件列表\n",
    "files = tf.train.match_filenames_once('data/data.tfrecords-*')\n",
    "\n",
    "# 创建文件输入队列\n",
    "filename_queue = tf.train.string_input_producer(files, shuffle=False)#再次注意，这里可以设置训练周期和shuffle\n",
    "\n",
    "# 读取并解析Example\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'i': tf.FixedLenFeature([], tf.int64),\n",
    "        'j': tf.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 使用match_filenames_once需要用local_variables_initializer初始化其中的一些变量，不然会出错\n",
    "    sess.run(\n",
    "        [tf.global_variables_initializer(),\n",
    "         tf.local_variables_initializer()])\n",
    "    '''或者聚合初始化(P85)\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                        tf.local_variables_initializer())#local的是使用协调器要求的\n",
    "    '''\n",
    "    # 打印文件名\n",
    "    print(sess.run(files))\n",
    "\n",
    "    # 用Coordinator协同线程，并启动线程\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # 获取数据\n",
    "    for i in range(6):\n",
    "        print(sess.run([features['i'], features['j']]))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 预处理\n",
    "- 对输入的样本进行任意的预处理， 这些预处理不依赖于训练参数， 在tensorflow/models/image/cifar10/cifar10.py可以找到数据归一化， 提取随机数据片，增加噪声或失真等等预处理的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips:\n",
    "- tf.train.match_filenames_once()中模式和linux一样：\n",
    "    - *:任意多字符  ？：任意字符一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取队列测试\n",
    "import tensorflow as tf\n",
    "\n",
    "directory = \"dataset\\\\data-*.TFRecord\"\n",
    "file_names = tf.train.match_filenames_once(directory)\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 至此，已经实现上面动图中的example Queue，在P87，作者提出实际应用时还需要一个batch Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 批处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建\n",
    "- 在数据输入管线的末端， 我们需要有另一个队列来执行输入样本的训练，评价和推理。因此我们使用tf.train.shuffle_batch 函数来对队列中的样本进行乱序处理(参考文档的用法，以及P86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    \"\"\"这部分就是前面解决的问题\"\"\"\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = some_decoder(record_string)\n",
    "    #数据预处理\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        filenames, num_epochs=num_epochs, shuffle=True\n",
    "    )\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    #以下为创建batch Queue，注意这里batch Queue类似插入在喂食到网络前的预处理层\n",
    "    min_after_dequeue = 1024\n",
    "    capcity = min_after_dequeue + 3*batch_size\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample(用于抽样的队列长度)\n",
    "    #        -- bigger means better shuffling but slower start up and more memory used.\n",
    "    # capacity 批数据队列容量\n",
    "    #        Recommendation(推荐):\n",
    "    #          min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size, capcity, min_after_dequeue\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这里还有一个类似的tf.train.shuffle_batch_join 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True\n",
    "    )\n",
    "    #使用多线程处理\n",
    "    example_list = [read_my_file_fromat(filename_queue)\n",
    "                   for _ in range(read_threads)]\n",
    "    min_after_dequeue = 1024\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batche = tf.train.shuffle_batch_join(\n",
    "        example_list, batch_size, capacity, min_after_dequeue\n",
    "    )\n",
    "    \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另一种替代方案是： 使用tf.train.shuffle_batch 函数,设置num_threads的值大于1。 这种方案可以保证同一时刻只在一个文件中进行读取操作(但是读取速度依然优于单线程)，而不是之前的同时读取多个文件。这种方案的优点是：\n",
    "\n",
    "    - 避免了两个不同的线程从同一个文件中读取同一个样本。\n",
    "    - 避免了过多的磁盘搜索操作。\n",
    "- 你一共需要多少个读取线程呢？ 函数tf.train.shuffle_batch*为TensorFlow图提供了获取文件名队列中的元素个数之和的方法。 如果你有足够多的读取线程， 文件名队列中的元素个数之和应该一直是一个略高于0的数。具体可以参考[TensorBoard:可视化学习](http://www.tensorfly.cn/tfdoc/how_tos/summaries_and_tensorboard.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型中的具体调用模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = input_pipline()\n",
    "with tf.Session as sess:\n",
    "    init_op = tf.group([\n",
    "        tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer()\n",
    "    ])\n",
    "    sess.run(init_op)\n",
    "    #协调器。现在代码有较好的封装\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #之前写项目直接八batch放进数据流图了。。。\n",
    "    try:\n",
    "        for _ in range(1000):\n",
    "            if not coord.should_stop():\n",
    "                batch = sess.run(x_batch)\n",
    "                sess.run(train_op, feed_dict={X: batch})\n",
    "                print('xx')\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('OutOfRangeError')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        print('Finish reading')\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用tf.data\n",
    "- [参考](https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247485929&idx=2&sn=1c76b725a5f941101cfe74d0a4e974a9&chksm=f9d15771cea6de6733c9ebdd6c5198a2ce6317367f596882773af680eaa50fd5e846cfd73559&mpshare=1&scene=23&srcid=0714u4Mpd2mZ53vuFUEzC3Cc#rd)\n",
    "- 使用tf.data.Dataset表示一个数据集合，集合的每个元素为一个batch\n",
    "- 使用tf.data.Iterator从数据集中提取数据，它本身是迭代器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从np.array\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(np.random.randn((5, 10))\n",
    "print(dataset1.output_types)# ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\" \n",
    "\n",
    "#从tensor\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((\n",
    "    tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "# 从文件\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset3 = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 支持多种链式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "dataset = dataset.map(...)  # 解析数据或者对数据预处理，如normalize.\n",
    "\n",
    "dataset = dataset.repeat()  # 重复数据集，一般设置num_epochs\n",
    "\n",
    "dataset = dataset.batch(32) # 形成batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建Iterator\n",
    "- 迭代器，4种类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-shot Iterator\n",
    "- 这是最简单的Iterator，它仅仅遍历整个数据集一次，而且不需要显示初始化，下面是个实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        sess.run(next_element) # 0, 1, ..., 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializable Iterator\n",
    "- 需要初始化，可以在每次初始化时传入不同参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, [])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "    for i in range(10):\n",
    "        print(sess.run(next_element)) # 0, 1, ..., 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reinitializable Iterator\n",
    "- 相比initializable Iterator，它可以支持从不同的Dataset进行初始化，有时候你需要训练集和测试集，但是两者并不同，此时就可以定义两个不同的Dataset，并配合reinitializable Iterator来定义一个通用的迭代器，在使用前只需要送入不同的Dataset进行初始化就可以，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.random.randn(100, 5)\n",
    "test_data = np.random.randn(20, 5)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "\n",
    "# 创建一个reinitializable iterator\n",
    "re_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                              train_dataset.output_shapes)\n",
    "next_element = re_iterator.get_next()\n",
    "train_init_op = re_iterator.make_initializer(train_dataset)\n",
    "test_init_op = re_iterator.make_initializer(test_dataset)\n",
    "with tf.Session() as sess:    # 训练\n",
    "    n_epochs = 2\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(train_init_op)\n",
    "        for j in range(100):\n",
    "            print(sess.run(next_element))\n",
    "    # 测试\n",
    "    sess.run(test_init_op)\n",
    "    for i in range(20):\n",
    "        print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feedable Iterator\n",
    "- 对于reinitializable iterator，它可以支持送入不同Dataset，从而完成数据集的切换，但是每次切换时必须要重新初始化。对于Feedable Iterator，其可以认为支持送入不同的Iterator，通过切换迭代器的string_handle来完成不同数据集的切换，并且在切换时迭代器的状态还会被保留，**这相比reinitializable iterator更加灵活**，下面是一个实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.random.randn(100, 5)\n",
    "val_data = np.random.randn(20, 5)\n",
    "n_epochs = 20train_dataset = tf.data.Dataset.from_tensor_slices(train_data).repeat(n_epochs)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_data)\n",
    "\n",
    "# 创建一个feedable iterator\n",
    "handle = tf.placeholder(tf.string, [])\n",
    "feed_iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types,\n",
    "                                                  train_dataset.output_shapes)\n",
    "next_element = feed_iterator.get_next()\n",
    "\n",
    "# 创建不同的iterator\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "val_iterator = val_dataset.make_initializable_iterator()\n",
    "with tf.Session() as sess:\n",
    "    # 生成对应的handle\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    # 训练\n",
    "    for n in range(n_epochs):\n",
    "        for i in range(100):\n",
    "            print(i, sess.run(next_element, feed_dict={handle: train_handle}))\n",
    "                # 验证\n",
    "        if n % 10 == 0:\n",
    "            sess.run(val_iterator.initializer)\n",
    "                    for i in range(20):\n",
    "                print(sess.run(next_element, feed_dict={handle: val_handle}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：（参见官方文档）\n",
    "-  在达到最大训练迭代数的时候如何清理关闭线程?\n",
    "- 筛选记录或产生每个记录的多个样本\n",
    "- 稀疏输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预加载数据\n",
    "- 这仅用于可以完全加载到存储器中的小的数据集。有两种方法：\n",
    "\n",
    "    - 存储在常数中。\n",
    "    - 存储在变量中，初始化后，永远不要改变它的值。\n",
    "- 使用常数更简单一些，但是会使用更多的内存（因为常数会内联的存储在数据流图数据结构中，这个结构体可能会被复制几次）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#常量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session():\n",
    "    input_data = tf.constant(training_data)\n",
    "    input_labels = tf.constant(training_labels)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#变量方法\n",
    "training_data = ...\n",
    "training_labels = ...\n",
    "with tf.Session() as sess:\n",
    "    data_initializer = tf.placeholder(dtype=training_data.dtype,\n",
    "                                    shape=training_data.shape)\n",
    "    label_initializer = tf.placeholder(dtype=training_labels.dtype,\n",
    "                                     shape=training_labels.shape)\n",
    "    input_data = tf.Variable(data_initalizer, trainable=False, collections=[])\n",
    "    input_labels = tf.Variable(label_initalizer, trainable=False, collections=[])\n",
    "    ...\n",
    "    sess.run(input_data.initializer,\n",
    "           feed_dict={data_initializer: training_data})\n",
    "    sess.run(input_labels.initializer,\n",
    "           feed_dict={label_initializer: training_lables})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设定trainable=False 可以防止该变量被数据流图的 GraphKeys.TRAINABLE_VARIABLES 收集, 这样我们就不会在训练的时候尝试更新它的值； 设定 collections=[] 可以防止GraphKeys.VARIABLES 收集后做为保存和恢复的中断点。\n",
    "\n",
    "- 无论哪种方式，[tf.train.slice_input_producer function](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#slice_input_producer)函数可以被用来每次产生一个切片。这样就会让样本在整个迭代中被打乱，所以在使用批处理的时候不需要再次打乱样本。所以我们不使用shuffle_batch函数，取而代之的是纯[tf.train.batch](http://www.tensorfly.cn/tfdoc/api_docs/python/io_ops.html#batch) 函数。 如果要使用多个线程进行预处理，需要将num_threads参数设置为大于1的数字。\n",
    "#### 例子\n",
    "- 在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded.py 中可以找到一个MNIST例子，使用常数来预加载。 另外使用变量来预加载的例子\n",
    "- 在tensorflow/g3doc/how_tos/reading_data/fully_connected_preloaded_var.py，你可以用上面 fully_connected_feed 和 fully_connected_reader 的描述来进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 多输入通道\n",
    "- 通常你会在一个数据集上面训练，然后在另外一个数据集上做评估计算(或称为 \"eval\")。 这样做的一种方法是，实际上包含两个独立的进程：\n",
    "\n",
    "    - 训练过程中读取输入数据，并定期将所有的训练的变量写入还原点文件）。\n",
    "    - 在计算过程中恢复还原点文件到一个推理模型中，读取有效的输入数据。\n",
    "- 这两个进程在下面的例子中已经完成了：the example CIFAR-10 model，有以下几个好处：\n",
    "\n",
    "    - eval被当做训练后变量的一个简单映射。\n",
    "    - 你甚至可以在训练完成和退出后执行eval。\n",
    "- 您可以在同一个进程的相同的数据流图中有训练和eval，并分享他们的训练后的变量。参考the shared variables tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
